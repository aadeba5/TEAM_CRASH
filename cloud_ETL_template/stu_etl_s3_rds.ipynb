{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6pz7LGh_L1p"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: apt-get\n",
      "zsh:1: command not found: wget\n",
      "tar: Error opening archive: Failed to open 'spark-3.0.2-bin-hadoop2.7.tgz'\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-64133a580fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Start a SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.2'\n",
    "spark_version = 'spark-3.0.2'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIGU4Tzs_Q4g"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "# Load in employee.csv from S3 into a DataFrame\n",
    "url = \"https://<bucket name>.s3.amazonaws.com/employee.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"employee.csv\"), inferSchema=True, sep=',', timestampFormat=\"m/dd/yy\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdtKPZ1w_V3e"
   },
   "source": [
    "## Drop duplicates and incomplete rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10qoqQv3_Y_w"
   },
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "df = df.dropna()\n",
    "print(df.count())\n",
    "df = df.dropDuplicates()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAgvBYkG_awE"
   },
   "source": [
    "## Examine the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXtPi-Hw_dCe"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klopnaUE_eZV"
   },
   "source": [
    "## Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9b0dvAat_f7L"
   },
   "outputs": [],
   "source": [
    "df1 = df.withColumnRenamed(\"Employee ID\", \"employee_id\") \\\n",
    "        .withColumnRenamed(\"Email\", \"email\") \\\n",
    "        .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "        .withColumnRenamed(\"Hire Date\", \"hire_date\") \\\n",
    "        .withColumnRenamed(\"DOB\", \"dob\") \\\n",
    "        .withColumnRenamed(\"Encrypted Password\", \"password\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbG7CtSA_gfw"
   },
   "source": [
    "## Create a new DataFrame for employee info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "saJ1WiZb_ik6"
   },
   "outputs": [],
   "source": [
    "employee_personal_info = df1.select([\"employee_id\", \"email\", \"gender\", \"hire_date\", \"dob\"])\n",
    "employee_personal_info.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PDvBiVN_jGe"
   },
   "source": [
    "## Write DataFrame to RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOIS3viE_leJ"
   },
   "outputs": [],
   "source": [
    "# Configuration for RDS instance\n",
    "mode=\"append\"\n",
    "jdbc_url = \"jdbc:postgresql://<insert endpoint>:5432/my_data_class_db\"\n",
    "config = {\"user\":\"root\",\n",
    "          \"password\": \"<insert password>\",\n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6_O84CK_nV5"
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to table\n",
    "\n",
    "employee_personal_info.write.jdbc(url=jdbc_url, table='employee_personal_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Or1RnPr_nzd"
   },
   "source": [
    "## Create a new DataFrame for employee passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7mDI_Xg9_q8_"
   },
   "outputs": [],
   "source": [
    "employee_password = df1.select([\"employee_id\", \"password\"])\n",
    "employee_password.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCmoHROm_saU"
   },
   "source": [
    "## Write DataFrame to RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CNKz5FHg_uV9"
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to table\n",
    "\n",
    "employee_password.write.jdbc(url=jdbc_url, table='employee_password', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tvw--Ool_vju"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "solved_stu_etl_s3_rds.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python391jvsc74a57bd0b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5",
   "display_name": "Python 3.9.1 64-bit ('3.9')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "metadata": {
   "interpreter": {
    "hash": "b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}